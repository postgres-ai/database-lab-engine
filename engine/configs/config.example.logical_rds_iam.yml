# Copy this configuration to: ~/.dblab/engine/configs/server.yml
# Configuration reference guide: https://postgres.ai/docs/reference-guides/database-lab-engine-configuration-reference
server:
  verificationToken: "secret_token" # Primary auth token; can be empty (not recommended); for multi-user mode, use DBLab EE
  port: 2345 # API server port; default: "2345"
  disableConfigModification: false # When true, configuration changes via API/CLI/UI are disabled; default: "false"

embeddedUI:
  enabled: true # If enabled, a separate UI container will be started
  dockerImage: "postgresai/ce-ui:latest" # Default: "postgresai/ce-ui:latest"
  host: "127.0.0.1" # Default: "127.0.0.1" (accepts only local connections)
  port: 2346 # UI port; default: "2346"

global:
  engine: postgres # Default: "postgres" (only Postgres is currently supported)
  debug: true # When true, more detailed logs are written to the server log
  database: # DB credentials used for management connections
    username: postgres # DB user, default: "postgres" (user must exist)
    dbname: postgres # DB name, default: "postgres" (DB must exist)

poolManager: # Manages filesystem pools (ZFS) or volume groups (LVM)
  mountDir: /var/lib/dblab # Pool mount directory; can contain multiple pools; default: "/var/lib/dblab"
  dataSubDir: data  # The "golden copy" data directory location, relative to mountDir; must exist; default: "data"
                    # Example: for "/var/lib/dblab/dblab_pool/data" set mountDir: "/var/lib/dblab" and dataSubDir: "data" (assuming mount point is "/var/lib/dblab/dblab_pool")
  clonesMountSubDir: clones # Where clones are mounted, relative to mountDir; default: "clones"
                            # Example: for "/var/lib/dblab/dblab_pool/clones" set mountDir: "/var/lib/dblab" and clonesMountSubDir: "clones" (assuming mount point is "/var/lib/dblab/dblab_pool"), resulting path for a clone running on port 6000: "/var/lib/dblab/dblab_pool/clones/6000"
  socketSubDir: sockets # Where sockets are located, relative to mountDir; default: "sockets"
  observerSubDir: observer # Where observability artifacts are located, relative to clone's data directory; default: "observer"
  preSnapshotSuffix: "_pre" # Suffix for preliminary snapshots; default: "_pre"
  selectedPool: "" # Force selection of working pool inside mountDir; default: "" (standard selection and rotation mechanism will be applied)

databaseContainer: &db_container  # Docker config for all DB containers
                                  # See https://postgres.ai/docs/database-lab/supported_databases
                                  # DBLab SE and EE customers get images compatible with RDS, RDS Aurora, GCP CloudSQL, Heroku, Timescale Cloud, Supabase, PostGIS
  dockerImage: "postgresai/extended-postgres:18-0.6.2" # Postgres image; major version (18) must match source if physical mode
  containerConfig: # Custom container config; see https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources
    "shm-size": 1gb # Shared memory size; increase if "could not resize shared memory segment" errors occur

databaseConfigs: &db_configs # Postgres config for all DB containers
  configs:
    shared_buffers: 1GB # Postgres buffer pool size; large values can lead to OOM
    shared_preload_libraries: "pg_stat_statements, pg_stat_kcache, auto_explain, logerrors" # Shared libraries; copy from source
    maintenance_work_mem: "500MB" # Maximum memory for maintenance operations (VACUUM, CREATE INDEX, etc.)
    work_mem: "100MB" # This and Query Planning parameters should be copied from source; see https://postgres.ai/docs/how-to-guides/administration/postgresql-configuration#postgresql-configuration-in-clones
    # ... put Query Planning parameters here

provision: # Defines how data is provisioned
  <<: *db_container
  portPool: # Range of ports for Postgres clones; ports will be allocated sequentially, starting from the lowest value
    from: 6000 # First port in the range
    to: 6099 # Last port in the range
  useSudo: false # Use sudo for ZFS/LVM and Docker commands if DBLab server running outside a container (not recommended)
  keepUserPasswords: false # Keep user passwords in clones; default: "false"
  cloneAccessAddresses: "127.0.0.1" # IP addresses that can be used to access clones; supports multiple IPs and IPv6; default: "127.0.0.1" (loop-back)

retrieval:  # Data retrieval: initial sync and ongoing updates. Two methods:
            #   - logical: dump/restore (works with RDS, different physical layout)
            #   - physical: direct copy (identical layout, not for RDS) e.g. using pg_basebackup, WAL-G, or pgBackRest
  refresh:
    timetable: "0 0 * * 1" # Full data refresh schedule in crontab format; see https://en.wikipedia.org/wiki/Cron#Overview
    skipStartRefresh: false # Skip data refresh while the retrieval starts
  jobs: # Jobs to run; must not contain physical and logical restore jobs simultaneously
    - logicalDump
    - logicalRestore
    - logicalSnapshot
  spec:
    logicalDump: # Dumps PostgreSQL database from provided source
      options:
        <<: *db_container
        dumpLocation: "/var/lib/dblab/dblab_pool/dump" # Dump file location; ensure enough disk space
        
        source:
          type: rdsIam # Source types: "local", "remote", "rdsIam"
          connection: # RDS database connection details for pg_dump
            dbname: test
            username: test_user
          rdsIam: # RDS IAM authentication configuration
            awsRegion: us-east-2 # AWS Region where RDS instance is located
            dbInstanceIdentifier: database-1 # RDS instance identifier
            sslRootCert: "/cert/rds-combined-ca-bundle.pem" # Path to SSL root certificate; download from https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem

        databases: # List of databases to dump; leave empty to dump all databases
        #   database1:
        #     tables: # Partial dump tables; corresponds to --table option of pg_dump
        #       - table1
        #       - table2
        #   database2:
        #   databaseN:

        parallelJobs: 4 # Parallel jobs for faster dump; ignored if immediateRestore.enabled is true
        
        # immediateRestore: # Direct restore to DBLab Engine instance; single-threaded unlike logicalRestore
        #   enabled: true
        #   <<: *db_configs # Adjust PostgreSQL configuration for logical dump job
        #   customOptions: # Custom options for pg_restore command
        #     - "--no-privileges"
        #     - "--no-owner"
        #     - "--exit-on-error"

        customOptions: # Custom options for pg_dump command
          - "--exclude-schema=rdsdms" # Exclude RDS DMS schema

    logicalRestore: # Restores PostgreSQL database from dump; don't use with immediateRestore
      options:
        <<: *db_container
        dumpLocation: "/var/lib/dblab/dblab_pool/dump" # Location of archive files to restore
        parallelJobs: 4 # Parallel jobs for faster restore
        <<: *db_configs # Adjust PostgreSQL configuration for logical restore job

        databases: # List of databases to restore; leave empty to restore all databases
        #   database1:
        #     format: directory # Dump format: directory, custom, plain; default: directory
        #     compression: no # Compression for plain-text dumps: gzip, bzip2, no; default: no
        #     tables: # Partial restore tables
        #       - table1
        #     excludeTables: # Exclude tables; corresponds to --exclude-table option of pg_dump
        #       - table2
        #   database2:
        #   databaseN:

        queryPreprocessing: # Pre-processing SQL queries
          queryPath: "" # Path to SQL pre-processing queries; default: empty (no pre-processing)
          maxParallelWorkers: 2 # Worker limit for parallel queries; doesn't work for inline SQL
          inline: "" # Inline SQL; runs after scripts in queryPath

        customOptions: # Custom options for pg_restore command
          - "--no-tablespaces"
          - "--no-privileges"
          - "--no-owner"
          - "--exit-on-error"
        
        skipPolicies: true # Skip policies during restore

    logicalSnapshot: # Final snapshot configuration
      options:
        <<: *db_configs # Adjust PostgreSQL configuration
        preprocessingScript: "" # Pre-processing script for data scrubbing/masking; e.g., "/tmp/scripts/custom.sh"
        
        dataPatching: # Pre-processing SQL queries for data patching
          <<: *db_container
          queryPreprocessing:
            queryPath: "" # Path to SQL pre-processing queries; default: empty
            maxParallelWorkers: 2 # Worker limit for parallel queries
            inline: "" # Inline SQL; runs after scripts in queryPath

cloning:
  accessHost: "localhost" # Host that will be specified in database connection info for all clones (only used to inform users)
  maxIdleMinutes: 120 # Automatically delete clones after the specified minutes of inactivity; 0 - disable automatic deletion

diagnostic:
  logsRetentionDays: 7 # How many days to keep logs

observer: # CI Observer configuration
#  replacementRules:  # Regexp rules for masking personal data in Postgres logs; applied before sending the logs to the Platform
#                     # Check the syntax of regular expressions: https://github.com/google/re2/wiki/Syntax
#    "regexp": "replace"
#    "select \\d+": "***"
#    "[a-z0-9._%+\\-]+(@[a-z0-9.\\-]+\\.[a-z]{2,4})": "***$1"

webhooks: # Webhooks can be used to trigger actions in external systems upon events such as clone creation
#  hooks:
#    - url: ""
#      secret: "" # (optional) Sent with the request in the `DBLab-Webhook-Token` HTTP header.
#      trigger:
#        - clone_create
#        - clone_reset

platform:
  url: "https://postgres.ai/api/general" # Default: "https://postgres.ai/api/general"
  enableTelemetry: true

#  ╔══════════════════════════════════════════════════════════════════════════╗
#  ║                     POSTGRES AI PLATFORM INTEGRATION                     ║
#  ╠══════════════════════════════════════════════════════════════════════════╣
#  ║                                                                          ║
#  ║  - Production-ready UI, AI assistance and support from human experts     ║
#  ║  - Enterprise-grade user management & role-based access control          ║
#  ║  - Advanced security: audit trails, SIEM integration, compliance         ║
#  ║  - Real-time performance monitoring & intelligent recommendations        ║
#  ║                                                                          ║
#  ║  Learn more at https://postgres.ai/                                      ║
#  ║                                                                          ║
#  ╚══════════════════════════════════════════════════════════════════════════╝
#
# Uncomment the following lines if you need the Platform integration
#
#  projectName: "project_name" # Project name
#  orgKey: "org_key" # Organization key
#  accessToken: "platform_access_token" # Token for authorization in Platform API; get it at https://postgres.ai/console/YOUR_ORG_NAME/tokens
#  enablePersonalTokens: true # Enable authorization with personal tokens of the organization's members.
#
